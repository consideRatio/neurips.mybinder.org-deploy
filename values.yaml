# Values for the parent chart, passed to subcharts

letsencrypt:
  contactEmail: erik.i.sundell@gmail.com

# Default values: https://github.com/jupyterhub/binderhub/blob/master/helm-chart/binderhub/values.yaml
# MyBinder.org values: https://github.com/jupyterhub/mybinder.org-deploy/blob/master/mybinder/values.yaml
binderhub:
  config:
    BinderHub:
      hub_url: 'https://hub.neurips.mybinder.org'
      use_registry: true
      image_prefix: gcr.io/binder-prod/neurips-

  service:
    type: ClusterIP

  ingress:
    enabled: true
    annotations:
      # cert-manager provides a TLS secret
      kubernetes.io/tls-acme: "true"
      # nginx-ingress controller to be explicitly utilized instead of "gce"
      kubernetes.io/ingress.class: nginx
    hosts:
      - neurips.mybinder.org
    tls:
      - secretName: neurips-mybinder-org-tls
        hosts:
          - neurips.mybinder.org

  # Default values: https://github.com/jupyterhub/zero-to-jupyterhub-k8s/blob/master/jupyterhub/values.yaml
  jupyterhub:
    cull:
      # cull every 11 minutes so it is out of phase
      # with the proxy check-routes interval of five minutes
      every: 660
      timeout: 600
      # maxAge is 6 hours: 6 * 3600 = 21600
      maxAge: 21600
    prePuller:
      continuous:
        enabled: true
      #using this as singleuser's image instead as that will always be pulled
      #extraImages:
      #  demo1:
      #    name: some-demo-image
      #    tag: a-tag
      #  demo2:
      #    name: some-demo-image
      #    tag: a-tag

    hub:
      extraConfig: {}
    proxy:
      service:
        type: ClusterIP
    ingress:
      enabled: true
      annotations:
        kubernetes.io/tls-acme: "true"
        kubernetes.io/ingress.class: nginx
        ingress.kubernetes.io/proxy-body-size: 64m
      hosts:
        - hub.neurips.mybinder.org
      tls:
        - secretName: hub-neurips-mybinder-org-tls
          hosts:
            - hub.neurips.mybinder.org
    scheduling:
      podPriority:
        enabled: true
      userScheduler:
        enabled: true
      userPlaceholder:
        enabled: true
        replicas: 0
      userPods:
        nodeAffinity:
          matchNodePurpose: prefer
      corePods:
        nodeAffinity:
          matchNodePurpose: require
    singleuser:
      # to allow a scaleup event to finish in worst case scenario
      # which will be required without placeholders
      startTimeout: 1200
      storage:
        # allows us to configure jupyter notebook
        extraVolumes:
          - name: etc-jupyter
            configMap:
              name: user-etc-jupyter
          - name: etc-jupyter-templates
            configMap:
              name: user-etc-jupyter-templates
        extraVolumeMounts:
          - name: etc-jupyter
            mountPath: /etc/jupyter
          - name: etc-jupyter-templates
            mountPath: /etc/jupyter/templates

      image:
        # this image is not supposed to be started, but it's
        # nice to get it pulled ahead of time to reduce pull
        # time of other images depending on it
        name: buildpack-deps
        tag: bionic
      memory:
        limit: 13G
        guarantee: 1G
      cpu:
        limit: 2
        guarantee: 0.05
      extraResource:
        limits:
          "nvidia.com/gpu": 1
        guarantees:
          "nvidia.com/gpu": 1

# these config files will be mounted on the user pods storage
etcJupyter:
  jupyter_notebook_config.json:
    NotebookApp:
      # shutdown the server after no activity
      shutdown_no_activity_timeout: 600

    # if a user leaves a notebook with a running kernel,
    # the effective idle timeout will typically be CULL_TIMEOUT + CULL_KERNEL_TIMEOUT
    # as culling the kernel will register activity,
    # resetting the no_activity timer for the server as a whole
    MappingKernelManager:
      # shutdown kernels after no activity
      cull_idle_timeout: 600
      # check for idle kernels this often
      cull_interval: 60
      # a kernel with open connections but no activity still counts as idle
      # this is what allows us to shutdown servers
      # when people leave a notebook open and wander off
      cull_connected: true

# Default values: https://github.com/helm/charts/blob/master/stable/nginx-ingress/values.yaml
nginx-ingress:
  controller:
    service:
      # I reserved an regional ip address in us-east1...
      # > gcloud compute addresses create neurips-mybinder-org --region us-east1
      # ... and inspected what address got ...
      # > gcloud compute addresses list
      # ... and assigned the IP to the field below
      loadBalancerIP: 35.190.132.172
  config:
    # Allow POSTs of upto 64MB, for large notebook support.
    proxy-body-size: 64m

# Default values: https://github.com/helm/charts/blob/master/stable/cert-manager/values.yaml
cert-manager:
  ingressShim:
    defaultIssuerName: "neurips"
    defaultIssuerKind: "ClusterIssuer"
    defaultACMEChallengeType: "http01"
